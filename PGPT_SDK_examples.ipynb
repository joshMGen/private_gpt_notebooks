{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import of private gpt sdk\n",
    "from pgpt_python.client import PrivateGPTApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching the sdk to the private gpt instance. by default private GPT should be at \"http://localhost:8001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attaching sdk to our private gpt instance\n",
    "client = PrivateGPTApi(base_url=\"http://172.31.37.42:8001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic command to check if PGPT is connected\n",
    "print(client.health.health())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code for ingesting files.\n",
    "with open(\"keyboard_test\", \"rb\") as f:\n",
    "    ingested_file_doc_id = client.ingestion.ingest_file(file=f).data[0].doc_id\n",
    "print(\"Ingested file doc id: \", ingested_file_doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here forward please feel free to edit the code and run the cells as many times as you want to and try to understand why the AI responds in different manners!\n",
    "\n",
    "It is highly encouraged to try out new prompts for the response code of variable lengths. Trying to prompt engineer to limit reply lengths, and how successful these soft guard rails are is another good feature to test. Testing how to display embeddings and see how they are effected by different strings may also be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## response code\n",
    "\n",
    "There are two types of responses:\n",
    "* non-streaming: Responses are generated by the AI and only sent out once the entire response is generated.\n",
    "* streaming: Responses begin to be generated and words are sent out in a stream as the AI determines the words.\n",
    " \n",
    "streaming excels in situations where the expected responses are short, do not need any string manipulations or when you need a way to show the user thier prompt has been read and is getting a responses even if slow. Where as batch or non-streaming responses excel, with long reponses, responses needing string manipuation or filtering ( to ensure nothing bad is said by the AI), or  for cost savings.\n",
    "\n",
    "**feel free to play around with the prompts for the AI, testing how length of sentence effect reply time or how gibberish may effect returned results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-streaming example \n",
    "prompt_result = client.contextual_completions.prompt_completion(\n",
    "    prompt=\"Answer with just the result: 2+2\"\n",
    ")\n",
    "print(prompt_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming example\n",
    "for i in client.contextual_completions.prompt_completion_stream(\n",
    "    prompt=\"Answer with just the result: 2+2\"\n",
    "):\n",
    "    print(i.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings can be useful to help pinpoin exactly which model you should use as well as a tracker to determine what scenarios may lead to the AI saying something it shouldn't. Checking on these embeddings is rare for a non AI professional but still a useful feature to have for trouble shooting. as well as ensuring limiters on embedding weights to reduce undesired AI generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example call for embeddings with specific message.\n",
    "embedding_result = client.embeddings.embeddings_generation(input=\"Hello world\")\n",
    "print(embedding_result.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
